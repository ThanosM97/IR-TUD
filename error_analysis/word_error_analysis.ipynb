{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pyserini.index import IndexReader\n",
    "from gensim.models import KeyedVectors\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER ='.'\n",
    "DATA_PATH = './data' # path to the data files\n",
    "OUTPUT_PATH = './outputs' # path to where the outputs will be saved\n",
    "GLOVE_FILENAME = DATA_PATH + 'glove.6B.300d.word2vec' #path to the pretrained glove embeddings in word2vec format\n",
    "GLOVE_RANKINGS_FILE = 'XGB_glove_l2r_ranking.txt' # path to glove rankings\n",
    "BM25_PRF_RANKINGS_FILE = 'prf.txt' # path to bm25+prf rankings\n",
    "GLOVE_EVAL_FILE = 'glove_l2r_errors.txt' # path to ndcg errors of glove rankins\n",
    "PATH_TO_INDEXES = 'indexes/trec-19-dl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behaviour on most difficult queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "qrels = pd.read_csv(DATA_PATH + '2019qrels-pass.txt',sep=' ', header=None, names = [\"query_id\", \"Q0\", \"passage_id\", \"relevance\"])\n",
    "test_queries = pd.read_csv(DATA_PATH + 'msmarco-test2019-queries.tsv',sep='\\t', header=None, names = [\"query_id\", \"query\"])\n",
    "results = pd.read_csv(DATA_PATH + GLOVE_RANKINGS_FILE,sep='\\t', header=None, names = [\"query_id\", \"passage_id\", \"rank\"])\n",
    "errors = pd.read_csv(DATA_PATH + GLOVE_EVAL_FILE,sep='\\t', header=None, names = [\"label\", \"query_id\", \"value\"])\n",
    "passages = pd.read_csv(DATA_PATH + 'collection.tsv',sep='\\t', header=None, names = [\"passage_id\", \"passage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make joins to take query and passage contents\n",
    "qrels = pd.merge(qrels, test_queries,  how='left', left_on=['query_id'], right_on = ['query_id'])\n",
    "qrels = pd.merge(qrels, passages,  how='left', left_on=['passage_id'], right_on = ['passage_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most difficult queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg = errors['label'][1]\n",
    "#get queries with ndcg <= 0.25, which will be the examined queries\n",
    "queries = errors.loc[(errors['value'] <= 0.3) & (errors['label'] == ndcg)]['query_id'].tolist()\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returned = results[results['query_id'].astype(str).isin(queries)].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find irrelevant passages returned for the most difficult queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find returned\n",
    "returned = results[results['query_id'].astype(str).isin(queries)].reset_index()\n",
    "# Find irrelevant (relevance = 0)\n",
    "irrelevant = qrels.loc[(qrels['relevance'] == 0)].reset_index()\n",
    "# Inner join to find irrelevant passages that were returned\n",
    "irrelevant_returned = pd.merge(returned, irrelevant,  how='inner', left_on=['query_id','passage_id'], right_on = ['query_id','passage_id']).drop(['index_x', 'index_y', 'Q0','relevance'], axis=1)\n",
    "irrelevant_returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find relevant passages not returned for the most difficult queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find returned passage ids\n",
    "returned_passages = results[results['query_id'].astype(str).isin(queries)].reset_index()[\"passage_id\"].tolist()\n",
    "# Find relevant (relevance >= 1) from examined queries\n",
    "relevant = qrels.loc[(qrels['relevance'] >= 1) & (qrels['query_id'].astype(str).isin(queries))].reset_index()\n",
    "# Find relevant that were not returned\n",
    "relevant_not_returned = relevant[~relevant['passage_id'].isin(returned_passages)].reset_index().drop(['level_0', 'index', 'Q0'], axis=1)\n",
    "relevant_not_returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Output Files\n",
    "irrelevant_returned.to_json(OUTPUT_PATH + \"glove_irrelevant_returned.json\", orient=\"records\", indent=2)\n",
    "relevant_not_returned.to_json(OUTPUT_PATH + \"glove_relevant_not_returned.json\", orient=\"records\", indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find percentages of relevant, highly relevant, and perfeclty relevant passages retreived by the model for the most difficult queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    available = qrels.loc[(qrels['relevance'] <= 3)].reset_index() \n",
    "    irrelevant = qrels.loc[(qrels['relevance'] <= i)].reset_index() \n",
    "    irrelevant\n",
    "\n",
    "    # Inner join to find irrelevant passages that were returned\n",
    "    irrelevant_returned = pd.merge(returned, irrelevant,  how='inner', left_on=['query_id','passage_id'], right_on = ['query_id','passage_id']).drop(['index_x', 'index_y', 'Q0','relevance'], axis=1)\n",
    "    available_returned = pd.merge(returned, available,  how='inner', left_on=['query_id','passage_id'], right_on = ['query_id','passage_id']).drop(['index_x', 'index_y', 'Q0','relevance'], axis=1)\n",
    "\n",
    "    success_rates = {}\n",
    "    for query in queries:\n",
    "        temp = available_returned.loc[(available_returned['query_id'] == int(query))].shape[0]\n",
    "        success_rates[query] = (temp - irrelevant_returned.loc[(irrelevant_returned['query_id'] == int(query))].shape[0])/temp\n",
    "    print(np.array(list(success_rates.values())).mean())\n",
    "    \n",
    "    # Create Output Files\n",
    "    with open(OUTPUT_PATH + \"glove_success_rates_greater_than_{}.json\".format(i+1), \"w\") as outfile:\n",
    "        json.dump(success_rates, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "qrels = pd.read_csv(DATA_PATH + '2019qrels-pass.txt',sep=' ', header=None, names = [\"query_id\", \"Q0\", \"passage_id\", \"relevance\"])\n",
    "test_queries = pd.read_csv(DATA_PATH + 'msmarco-test2019-queries.tsv',sep='\\t', header=None, names = [\"query_id\", \"query\"])\n",
    "results = pd.read_csv(DATA_PATH + GLOVE_RANKINGS_FILE,sep='\\t', header=None, names = [\"query_id\", \"passage_id\", \"rank\"])\n",
    "errors = pd.read_csv(DATA_PATH + GLOVE_EVAL_FILE,sep='\\t', header=None, names = [\"label\", \"query_id\", \"value\"])\n",
    "passages = pd.read_csv(DATA_PATH + 'collection.tsv',sep='\\t', header=None, names = [\"passage_id\", \"passage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make joins to take query and passage contents\n",
    "qrels = pd.merge(qrels, test_queries,  how='left', left_on=['query_id'], right_on = ['query_id'])\n",
    "qrels = pd.merge(qrels, passages,  how='left', left_on=['passage_id'], right_on = ['passage_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find highly relevant, long passages for the most difficult queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = qrels[qrels['query_id'].astype(str).isin(queries) & (qrels['relevance'] == 2)].reset_index()\n",
    "# keep passages with more than 700 characters\n",
    "mask = (temp['passage'].str.len() > 700)\n",
    "df = temp.loc[mask].reset_index().drop([\"level_0\"], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(GLOVE_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordVectorizer to convert word embeddings into passage embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecVectorizer:\n",
    "  def __init__(self, model):\n",
    "    print(\"Loading in word vectors...\")\n",
    "    self.word_vectors = model\n",
    "    print(\"Finished loading in word vectors\")\n",
    "\n",
    "  def transform(self, data):\n",
    "    # determine the dimensionality of vectors\n",
    "    v = self.word_vectors.get_vector('king')\n",
    "    self.D = v.shape[0]\n",
    "\n",
    "    X = np.zeros((len(data), self.D))\n",
    "    n = 0\n",
    "    emptycount = 0\n",
    "    for sentence in data:\n",
    "      tokens = sentence.split()\n",
    "      vecs = []\n",
    "      m = 0\n",
    "      for word in tokens:\n",
    "        try:\n",
    "          # throws KeyError if word not found\n",
    "          vec = self.word_vectors.get_vector(word)\n",
    "          vecs.append(vec)\n",
    "          m += 1\n",
    "        except KeyError:\n",
    "          pass\n",
    "      if len(vecs) > 0:\n",
    "        vecs = np.array(vecs)\n",
    "        X[n] = vecs.mean(axis=0)\n",
    "      else:\n",
    "        emptycount += 1\n",
    "      n += 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distances between query and passage embedding representations, and between query and most similar sentence in the passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vectorizer\n",
    "vectorizer = Word2VecVectorizer(model)\n",
    "for i in range(df.shape[0]):\n",
    "    # generate query and passage embeddings\n",
    "    q_embds = pd.DataFrame(vectorizer.transform([df.at[i, \"query\"]])).iloc[0]\n",
    "    p_embds = pd.DataFrame(vectorizer.transform([df.at[i, \"passage\"]])).iloc[0]\n",
    "\n",
    "    # calculate cosine and euclidean distances and add as respective columns\n",
    "    df.at[i, \"passage_cosine_distance\"] = dot(p_embds, q_embds)/(norm(p_embds)*norm(q_embds))\n",
    "    df.at[i, \"passage_euclidean_distance\"] =  np.linalg.norm(p_embds.to_numpy()-q_embds.to_numpy())\n",
    "\n",
    "    df.at[i, \"min_sentence_cosine_distance\"] = 2\n",
    "    df.at[i, \"min_sentence_euclidean_distance\"] = 1000\n",
    "    # Split passage into sentences\n",
    "    for sentence in df.at[i, \"passage\"].split(\".\"):\n",
    "        s_embds = pd.DataFrame(vectorizer.transform([sentence])).iloc[0]\n",
    "        # Find sentence with minimum cosine distance\n",
    "        if 1 - dot(p_embds, s_embds)/(norm(p_embds)*norm(s_embds)) < df.at[i, \"min_sentence_cosine_distance\"]:\n",
    "            # Store distance and sentence as respective columns\n",
    "            df.at[i, \"min_sentence_cosine_distance\"] = 1 - dot(p_embds, s_embds)/(norm(p_embds)*norm(s_embds))\n",
    "            df.at[i, \"min_cosine_sentence\"] = sentence\n",
    "\n",
    "        # Find sentence with minimum cosine distance\n",
    "        if np.linalg.norm(p_embds.to_numpy()-s_embds.to_numpy()) < df.at[i, \"min_sentence_euclidean_distance\"]:\n",
    "            # Store distance and sentence as respective columns\n",
    "            df.at[i, \"min_sentence_euclidean_distance\"] = np.linalg.norm(p_embds.to_numpy()-s_embds.to_numpy())\n",
    "            df.at[i, \"min_euclidean_sentence\"] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"index\",\"query_id\",\"Q0\",\"passage_id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(OUTPUT_PATH + \"glove_sentence_analysis.txt\", header=True, sep=\" \", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passage length analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only large passages, with more than 700 characters\n",
    "mask = (qrels['passage'].str.len() > 700)\n",
    "qres_large = qrels.loc[mask].reset_index()\n",
    "qres_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_embedd = pd.read_csv(GLOVE_RANKINGS_FILE, sep='\\t', header=None, names = [\"query_id\", \"passage_id\", \"rank\"])\n",
    "results_prf = pd.read_csv(BM25_PRF_RANKINGS_FILE, sep='\\t', header=None, names = [\"query_id\", \"passage_id\", \"rank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make joins to take query and passage contents\n",
    "df = pd.merge(qres_large, results_prf,  how='left', left_on=['query_id', 'passage_id'], right_on = ['query_id', 'passage_id']).rename(columns={\"rank\": \"PRF_rank\"})\n",
    "df = pd.merge(df, results_embedd,  how='left', left_on=['query_id', 'passage_id'], right_on = ['query_id', 'passage_id']).rename(columns={\"rank\": \"embed_rank\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only highly relevant queries, which were ranked in the top 10 positions of BM25+PRF and study the relation between passage length and the new rankings from the GloVe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[(df['relevance'] >= 2) & (df['PRF_rank'] <= 10)].reset_index().drop([\"index\",\"query_id\",\"Q0\",\"passage_id\",\"level_0\"], axis=1)\n",
    "for i in range(df.shape[0]):\n",
    "    df.at[i,\"passage_length\"] = len(df.at[i,'passage'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(OUTPUT_PATH + \"glove_passage_length_analysis.txt\", header=True, sep=\" \", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
