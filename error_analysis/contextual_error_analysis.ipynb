{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER ='.'\n",
    "DATA_PATH = './data' # path to the data files\n",
    "OUTPUT_PATH = './outputs' # path to where the outputs will be saved\n",
    "DISTILBERT_RANKINGS_FILE = 'distilbert_cosine_similarity.txt' # path to Distlbert rankings\n",
    "BM25_PRF_RANKINGS_FILE = 'prf.txt' # path to bm25+prf rankings\n",
    "DISTILBERT_EVAL_FILE = 'errors_distilbert_ranking.txt' # path to ndcg errors of Distlbert rankins\n",
    "PATH_TO_INDEXES = 'indexes/trec-19-dl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behaviour on most difficult queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "qrels = pd.read_csv(DATA_PATH + '2019qrels-pass.txt',sep=' ', header=None, names = [\"query_id\", \"Q0\", \"passage_id\", \"relevance\"])\n",
    "test_queries = pd.read_csv(DATA_PATH + 'msmarco-test2019-queries.tsv',sep='\\t', header=None, names = [\"query_id\", \"query\"])\n",
    "results = pd.read_csv(DATA_PATH + DISTILBERT_RANKINGS_FILE,sep='\\t', header=None, names = [\"query_id\", \"passage_id\", \"rank\"])\n",
    "errors = pd.read_csv(DATA_PATH + DISTILBERT_EVAL_FILE,sep='\\t', header=None, names = [\"label\", \"query_id\", \"value\"])\n",
    "passages = pd.read_csv(DATA_PATH + 'collection.tsv',sep='\\t', header=None, names = [\"passage_id\", \"passage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make joins to take query and passage contents\n",
    "qrels = pd.merge(qrels, test_queries,  how='left', left_on=['query_id'], right_on = ['query_id'])\n",
    "qrels = pd.merge(qrels, passages,  how='left', left_on=['passage_id'], right_on = ['passage_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most difficult queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg = errors['label'][1]\n",
    "#get queries with ndcg <= 0.25, which will be the examined queries\n",
    "queries = errors.loc[(errors['value'] <= 0.3) & (errors['label'] == ndcg)]['query_id'].tolist()\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returned = results[results['query_id'].astype(str).isin(queries)].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find irrelevant passages returned for the most difficult queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find returned\n",
    "returned = results[results['query_id'].astype(str).isin(queries)].reset_index()\n",
    "# Find irrelevant (relevance = 0)\n",
    "irrelevant = qrels.loc[(qrels['relevance'] == 0)].reset_index()\n",
    "# Inner join to find irrelevant passages that were returned\n",
    "irrelevant_returned = pd.merge(returned, irrelevant,  how='inner', left_on=['query_id','passage_id'], right_on = ['query_id','passage_id']).drop(['index_x', 'index_y', 'Q0','relevance'], axis=1)\n",
    "irrelevant_returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find relevant passages not returned for the most difficult queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find returned passage ids\n",
    "returned_passages = results[results['query_id'].astype(str).isin(queries)].reset_index()[\"passage_id\"].tolist()\n",
    "# Find relevant (relevance >= 1) from examined queries\n",
    "relevant = qrels.loc[(qrels['relevance'] >= 1) & (qrels['query_id'].astype(str).isin(queries))].reset_index()\n",
    "# Find relevant that were not returned\n",
    "relevant_not_returned = relevant[~relevant['passage_id'].isin(returned_passages)].reset_index().drop(['level_0', 'index', 'Q0'], axis=1)\n",
    "relevant_not_returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Output Files\n",
    "irrelevant_returned.to_json(OUTPUT_PATH + \"distilbert_irrelevant_returned.json\", orient=\"records\", indent=2)\n",
    "relevant_not_returned.to_json(OUTPUT_PATH + \"distilbert_relevant_not_returned.json\", orient=\"records\", indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find percentages of relevant, highly relevant, and perfeclty relevant passages retreived by the model for the most difficult queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    available = qrels.loc[(qrels['relevance'] <= 3)].reset_index() \n",
    "    irrelevant = qrels.loc[(qrels['relevance'] <= i)].reset_index() \n",
    "    irrelevant\n",
    "\n",
    "    # Inner join to find irrelevant passages that were returned\n",
    "    irrelevant_returned = pd.merge(returned, irrelevant,  how='inner', left_on=['query_id','passage_id'], right_on = ['query_id','passage_id']).drop(['index_x', 'index_y', 'Q0','relevance'], axis=1)\n",
    "    available_returned = pd.merge(returned, available,  how='inner', left_on=['query_id','passage_id'], right_on = ['query_id','passage_id']).drop(['index_x', 'index_y', 'Q0','relevance'], axis=1)\n",
    "\n",
    "    success_rates = {}\n",
    "    for query in queries:\n",
    "        temp = available_returned.loc[(available_returned['query_id'] == int(query))].shape[0]\n",
    "        success_rates[query] = (temp - irrelevant_returned.loc[(irrelevant_returned['query_id'] == int(query))].shape[0])/temp\n",
    "    print(np.array(list(success_rates.values())).mean())\n",
    "    \n",
    "    # Create Output Files\n",
    "    with open(OUTPUT_PATH + \"glove_success_rates_greater_than_{}.json\".format(i+1), \"w\") as outfile:\n",
    "        json.dump(success_rates, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passage length analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "qrels = pd.read_csv(DATA_PATH + '2019qrels-pass.txt',sep=' ', header=None, names = [\"query_id\", \"Q0\", \"passage_id\", \"relevance\"])\n",
    "test_queries = pd.read_csv(DATA_PATH + 'msmarco-test2019-queries.tsv',sep='\\t', header=None, names = [\"query_id\", \"query\"])\n",
    "results = pd.read_csv(DATA_PATH + DISTILBERT_RANKINGS_FILE,sep='\\t', header=None, names = [\"query_id\", \"passage_id\", \"rank\"])\n",
    "errors = pd.read_csv(DATA_PATH + DISTILBERT_EVAL_FILE,sep='\\t', header=None, names = [\"label\", \"query_id\", \"value\"])\n",
    "passages = pd.read_csv(DATA_PATH + 'collection.tsv',sep='\\t', header=None, names = [\"passage_id\", \"passage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make joins to take query and passage contents\n",
    "qrels = pd.merge(qrels, test_queries,  how='left', left_on=['query_id'], right_on = ['query_id'])\n",
    "qrels = pd.merge(qrels, passages,  how='left', left_on=['passage_id'], right_on = ['passage_id'])\n",
    "\n",
    "# Keep only large passages, with more than 700 characters\n",
    "mask = (qrels['passage'].str.len() > 700)\n",
    "qres_large = qrels.loc[mask].reset_index()\n",
    "qres_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_embedd = pd.read_csv(DISTILBERT_RANKINGS_FILE, sep='\\t', header=None, names = [\"query_id\", \"passage_id\", \"rank\"])\n",
    "results_prf = pd.read_csv(BM25_PRF_RANKINGS_FILE, sep='\\t', header=None, names = [\"query_id\", \"passage_id\", \"rank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make joins to take query and passage contents\n",
    "df = pd.merge(qres_large, results_prf,  how='left', left_on=['query_id', 'passage_id'], right_on = ['query_id', 'passage_id']).rename(columns={\"rank\": \"PRF_rank\"})\n",
    "df = pd.merge(df, results_embedd,  how='left', left_on=['query_id', 'passage_id'], right_on = ['query_id', 'passage_id']).rename(columns={\"rank\": \"embed_rank\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only highly relevant queries, which were ranked in the top 10 positions of BM25+PRF and study the relation between passage length and the new rankings from the GloVe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[(df['relevance'] >= 2) & (df['PRF_rank'] <= 10)].reset_index().drop([\"index\",\"query_id\",\"Q0\",\"passage_id\",\"level_0\"], axis=1)\n",
    "for i in range(df.shape[0]):\n",
    "    df.at[i,\"passage_length\"] = len(df.at[i,'passage'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save results\n",
    "df.to_csv(OUTPUT_PATH + \"distilbert_passage_length_analysis.txt\", header=True, sep=\" \", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
